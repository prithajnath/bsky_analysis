{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274a7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "os.environ['LDFLAGS'] = '-L/opt/homebrew/opt/libomp/lib'\n",
    "os.environ['CPPFLAGS'] = '-I/opt/homebrew/opt/libomp/include'\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "merged_df = pd.read_csv(\"merged_df.csv\")\n",
    "\n",
    "# Fill NA for has_mentions_in_bio and has_url_in_bio\n",
    "merged_df['has_mentions_in_bio'] = merged_df['has_mentions_in_bio'].fillna(0)\n",
    "merged_df['has_url_in_bio'] = merged_df['has_url_in_bio'].fillna(0)\n",
    "\n",
    "# Drop rows with any missing values\n",
    "merged_df_clean = merged_df.dropna()\n",
    "merged_df_clean\n",
    "\n",
    "# select features and target\n",
    "X = merged_df_clean.drop(columns=['kmeans_cluster', 'did', 'handle', 'first_post_body', 'bio', 'created_at', 'first_post_created_at'])\n",
    "y = merged_df_clean['kmeans_cluster']\n",
    "\n",
    "# normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "n_splits = 10\n",
    "# get folds\n",
    "k_folds = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "# define models\n",
    "\n",
    "# logistic regression\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "log_reg_bal = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# svc\n",
    "# svc = SVC(\n",
    "#     probability=True,\n",
    "#     kernel='poly'\n",
    "# )\n",
    "\n",
    "# svc_bal = SVC(\n",
    "#     probability=True,\n",
    "#     class_weight='balanced',\n",
    "#     kernel='poly'\n",
    "# )\n",
    "\n",
    "# random forest\n",
    "rf = RandomForestClassifier()\n",
    "rf_bal = RandomForestClassifier(\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# xgboost\n",
    "xgb = XGBClassifier(\n",
    "    eval_metric='aucpr'\n",
    ")\n",
    "\n",
    "# Initialize feature importance data\n",
    "feature_data = {}\n",
    "\n",
    "model_names = [\n",
    "    \"Logistic Regression\",\n",
    "    \"Logistic Regression (Balanced)\",\n",
    "    # \"SVC\",\n",
    "    # \"SVC (Balanced)\",\n",
    "    \"Random Forest\",\n",
    "    \"Random Forest (Balanced)\",\n",
    "    \"XGBoost\",\n",
    "    \"XGBoost (Balanced)\",\n",
    "    \"XGBoost (Lasso)\",\n",
    "    \"HGB Gradient Boosting Classifier\",\n",
    "    \"HGB Classifier (Balanced)\",\n",
    "    \"Ensemble (Voting Classifier)\",\n",
    "    \"Curated Ensemble\",\n",
    "]\n",
    "\n",
    "fold_scores = {i:{\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': [],\n",
    "    'ROC AUC Score': [],\n",
    "} for i in model_names}\n",
    "\n",
    "y_preds = {i:[] for i in model_names}\n",
    "y_vals = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(k_folds.split(X_scaled,y)):\n",
    "    X_train_scaled = X_scaled[train_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    X_val_scaled = X_scaled[test_index]\n",
    "    y_val = y.iloc[test_index]\n",
    "\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_train_scaled, y_train = smote.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    y_vals.extend(y_val.to_numpy())\n",
    "\n",
    "    # class imbalance ratios\n",
    "    scale = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "    sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "    \n",
    "    xgb_bal = XGBClassifier(\n",
    "        eval_metric='aucpr',\n",
    "        scale_pos_weight=scale)\n",
    "    xgb_lasso = XGBClassifier(\n",
    "        eval_metric='aucpr',\n",
    "        scale_pos_weight=scale,\n",
    "        reg_alpha=1.0  # Lasso\n",
    "    )\n",
    "\n",
    "    # Histogram Based Gradient Boosting\n",
    "    hgb = HistGradientBoostingClassifier()\n",
    "    hgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "    hgb_bal = HistGradientBoostingClassifier()\n",
    "    hgb_bal.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
    "\n",
    "    # Combine into ensemble\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', log_reg),\n",
    "            ('lr_bal', log_reg_bal),\n",
    "            # ('svc', svc),\n",
    "            # ('svc_bal', svc_bal),\n",
    "            ('rf', rf),\n",
    "            ('rf_bal', rf_bal),\n",
    "            ('xgb', xgb),\n",
    "            ('xgb_bal', xgb_bal),\n",
    "            ('xgb_lasso', xgb_lasso),\n",
    "            ('hgb', hgb),\n",
    "            ('hgb_bal', hgb_bal),\n",
    "        ],\n",
    "        voting='soft' # soft uses predict_proba - hard uses voting\n",
    "    )\n",
    "\n",
    "    # Ensemble with Histogram-based gradient boosting and Logistic Balanced\n",
    "    curated_ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr_bal', log_reg_bal),\n",
    "            ('hgb_bal', hgb_bal),\n",
    "            ('xgb_bal', xgb_bal),\n",
    "        ],\n",
    "        voting='soft' # soft uses predict_proba - hard uses voting\n",
    "    )\n",
    "\n",
    "    # define classifiers\n",
    "    models = {\n",
    "        \"Logistic Regression\": log_reg,\n",
    "        \"Logistic Regression (Balanced)\": log_reg_bal,\n",
    "        # \"SVC\": svc,\n",
    "        # \"SVC (Balanced)\": svc_bal,\n",
    "        \"Random Forest\": rf,\n",
    "        \"Random Forest (Balanced)\": rf_bal,\n",
    "        \"XGBoost\": xgb,\n",
    "        \"XGBoost (Balanced)\": xgb_bal,\n",
    "        \"XGBoost (Lasso)\": xgb_lasso,\n",
    "        \"HGB Gradient Boosting Classifier\": hgb,\n",
    "        \"HGB Classifier (Balanced)\": hgb_bal,\n",
    "        \"Ensemble (Voting Classifier)\": ensemble,\n",
    "        \"Curated Ensemble\": curated_ensemble\n",
    "    }\n",
    "    print(y.value_counts(normalize=True))\n",
    "\n",
    "    # train, predict, evaluate\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüîç {name} (fold {i})\")\n",
    "\n",
    "        # train\n",
    "        if not isinstance(model, HistGradientBoostingClassifier):\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # predict\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        y_preds[name].extend(y_pred)\n",
    "\n",
    "        # evaluate\n",
    "        fold_scores[name][\"Accuracy\"].append(accuracy_score(y_val, y_pred))\n",
    "        fold_scores[name][\"Precision\"].append(precision_score(y_val, y_pred, average='binary', zero_division=0))\n",
    "        fold_scores[name][\"Recall\"].append(recall_score(y_val, y_pred, average='binary', zero_division=0))\n",
    "        fold_scores[name][\"F1 Score\"].append(f1_score(y_val, y_pred, average='binary', zero_division=0))\n",
    "        fold_scores[name][\"ROC AUC Score\"].append(roc_auc_score(y_val, y_pred))\n",
    "\n",
    "        # full classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_val, y_pred))\n",
    "\n",
    "        # Feature importance (if available)\n",
    "        try:\n",
    "            # Tree-based models\n",
    "            if hasattr(model, \"feature_importances_\"): \n",
    "                importances = model.feature_importances_\n",
    "            # Linear models\n",
    "            elif hasattr(model, \"coef_\"):\n",
    "                importances = model.coef_[0]\n",
    "            else:\n",
    "                continue  # skip if no importance measure\n",
    "\n",
    "            feature_data[name] = importances\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in model_names:\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_vals, y_preds[name])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "# Collect metrics\n",
    "metrics = {\n",
    "    'Model': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': [],\n",
    "    'ROC AUC Score': []\n",
    "}\n",
    "\n",
    "# convert to dataframe\n",
    "for model in fold_scores:\n",
    "    metrics['Model'].append(model)\n",
    "    metrics['Accuracy'].append(np.mean(fold_scores[model]['Accuracy'])),\n",
    "    metrics['Precision'].append(np.mean(fold_scores[model]['Precision'])),\n",
    "    metrics['Recall'].append(np.mean(fold_scores[model]['Recall'])),\n",
    "    metrics['F1 Score'].append(np.mean(fold_scores[model]['F1 Score'])),\n",
    "    metrics['ROC AUC Score'].append(np.mean(fold_scores[model]['ROC AUC Score']))\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# convert feature importances to dataFrame\n",
    "feature_df = pd.DataFrame(feature_data, index=X.columns)\n",
    "\n",
    "# sort columns\n",
    "feature_df = feature_df.sort_index()\n",
    "\n",
    "# export\n",
    "feature_df.to_csv(\"model_feature_importances.csv\")\n",
    "print(\"Exported to model_feature_importances.csv\")\n",
    "\n",
    "# plot\n",
    "ax = metrics_df.set_index('Model').plot(kind='bar', figsize=(12, 6))\n",
    "plt.title(\"Classifier Comparison on Evaluation Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y')\n",
    "ax.legend(loc='lower left', bbox_to_anchor=(0, 1.02), ncol=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python mlkernel",
   "language": "python",
   "name": "mlkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
